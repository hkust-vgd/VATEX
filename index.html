<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <meta charset="utf-8">
  <meta name="description"
        content="VATEX">
  <meta name="keywords" content="VATEX, RIS, Vision-Aware Text Features, Referring Image Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding</title>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .abstract-container {
      animation: fadeIn 1s ease-in;
      background: linear-gradient(135deg, #f5f7fa 0%, #e8ecf1 100%);
      padding: 2rem;
      border-radius: 15px;
      box-shadow: 0 4px 15px rgba(0,0,0,0.1);
      margin: 2rem 0;
    }

    .highlight-box {
      background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
      padding: 1.5rem;
      margin: 2rem 0;
      border-left: 5px solid #4a90e2;
      border-radius: 8px;
      transition: transform 0.3s ease;
    }

    .highlight-box:hover {
      transform: translateY(-5px);
    }

    .feature-box {
      background: #fff;
      padding: 1.5rem;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
      transition: all 0.3s ease;
    }

    .feature-box:hover {
      transform: translateY(-5px);
      box-shadow: 0 6px 15px rgba(0,0,0,0.15);
    }

    .features-container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1.5rem;
      margin: 2rem 0;
    }

    @media (max-width: 768px) {
      .features-container {
        grid-template-columns: 1fr;
      }
    }

    .object-feature {
      background: linear-gradient(135deg, #fff5f0 0%, #fff 100%);
      border-left: 4px solid #C55A11;
    }

    .context-feature {
      background: linear-gradient(135deg, #f0f5ff 0%, #fff 100%);
      border-left: 4px solid #0570C0;
    }

    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(20px); }
      to { opacity: 1; transform: translateY(0); }
    }
  </style>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-spaced is-1 publication-title">Vision-Aware Text Features in Referring Image Segmentation:<br>From Object Understanding to Context Understanding</h1>
          <h2 class="subtitle is-3" style="color: #ff3860; margin-bottom: 12px; margin-top: -12px; font-weight: bold; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); animation: fadeIn 1s ease-out;">WACV 2025</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/itruonghai">Hai Nguyen-Truong</a><sup>1,*</sup>,</span>
            <span class="author-block">
              <a href="https://nero1342.github.io">E-Ro Nguyen</a><sup>2,3,5,*</sup>,</span>
            <span class="author-block">
              <a href="/">Tuan-Anh Vu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="/">Minh-Triet Tran</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="/">Binh-Son Hua</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="/">Sai-Kit Yeung</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology</span>
            <span class="author-block"><sup>2</sup>University of Science, VNU-HCM, Ho Chi Minh city, Vietnam</span>
            <span class="author-block"><sup>3</sup>Viet Nam National University, Ho Chi Minh city, Vietnam</span>
            <span class="author-block"><sup>4</sup>Trinity College Dublin, Ireland</span>
            <span class="author-block"><sup>5</sup>Stony Brook University, New York, USA</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              
              <span class="link-block">
                <a href="https://openaccess.thecvf.com/content/WACV2025/html/Nguyen-Truong_Vision-Aware_Text_Features_in_Referring_Image_Segmentation_From_Object_Understanding_WACV_2025_paper.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.08590"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/nero1342/VATEX"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Poster Link -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1pQ94S2Lcll7Gm2byyBb_7jElFhbF_Lqi/view?usp=drive_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-image"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="empty">
      <h2 class="subtitle has-text-centered">
        <br>
        <span>
          Qualitative comparison between LAVT and Ours. The yellow box indicates the wrong segmentation results. 
          <span style="color: #C55A11">Object understanding</span> and 
          <span style="color: #0570C0">Context understanding</span> 
          are required to tackle the challenge of complex and ambiguous language expression.
        </span>
      </h2>
      <h2 class="subtitle has-text-centered">
        <br>
        <strong style="color: orange;font-size: 1.2em">TL;DR:</strong>
        <i><span style="font-size: 1.0em; font-weight: bold">VATEX</span> is a novel method for referring image segmentation that leverages vision-aware text features to improve text understanding. By decomposing language cues into object and context understanding, the model can better localize objects and interpret complex sentences, leading to significant performance gains.
        </i>
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 animate__animated animate__fadeIn">Abstract</h2>
        <div class="content has-text-justified">
          <div class="abstract-container">
            <p>
              Referring image segmentation is a challenging task that involves generating pixel-wise segmentation masks based on natural language descriptions. 
              The complexity of this task increases with the intricacy of the sentences provided.
            </p>
            
            <p>
              Existing methods have relied mostly on visual features to generate the segmentation masks while treating text features as supporting components. 
              However, this under-utilization of text understanding limits the model's capability to fully comprehend the given expressions.
            </p>

            <div class="highlight-box">
              <p style="font-weight: bold; color: #333; font-size: 1.2em; margin: 0;">
                In this work, we propose a novel framework that specifically emphasizes 
                <span style="color: #C55A11; font-weight: 700;">object</span> and 
                <span style="color: #0570C0; font-weight: 700;">context</span> 
                comprehension inspired by human cognitive processes through Vision-Aware Text Features.
              </p>
            </div>

            <div class="features-container">
              <div class="feature-box object-feature">
                <h4 style="color: #C55A11; font-weight: 600; font-size: 1.2em;">
                  <i class="fas fa-bullseye"></i> Object Understanding
                </h4>
                <!-- <h5 style="color: #C55A11; font-size: 1em;"> -->
                  <!-- <span style="background: linear-gradient(120deg, rgba(197,90,17,0.1) 0%, rgba(197,90,17,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">CLIP Prior</span> -->
                <!-- </h5> -->
                <p>
                  We introduce a <span style="color: #C55A11; font-weight: 600;background: linear-gradient(120deg, rgba(197,90,17,0.1) 0%, rgba(197,90,17,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">CLIP Prior</span> module to localize the main object of interest and embed the object heatmap into the query initialization process.
                </p>
              </div>

              <div class="feature-box context-feature">
                <h4 style="color: #0570C0; font-weight: 600; font-size: 1.2em;">
                  <i class="fas fa-project-diagram"></i> Context Understanding
                </h4>
                <!-- <h5 style="color: #0570C0; font-size: 1em;">CMD and MCC</h5> -->
                <p>
                  We propose a combination of two components: 
                  <span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">Contextual Multimodal Decoder (CMD)</span> and 
                  <span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">Meaning Consistency Constraint (MCC)</span>, 
                  to further enhance the coherent and consistent interpretation of language cues.
                </p>
              </div>
            </div>

            <div style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px; margin-top: 2rem;">
              <p style="margin: 0;">
                <span style="color: #2ea44f; font-weight: 600;">✨ Results:</span> 
                Our method achieves significant performance improvements on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref.
                <br><br>
                <span style="color: #0366d6; font-weight: 600;">🚀 Code Released:</span> 
                Our code and pre-trained weights are available at <a href="https://github.com/nero1342/VATEX" target="_blank" style="color: #0366d6; font-weight: 600;">https://github.com/nero1342/VATEX</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/-uGaBHqbBd0?si=bi82-hq265HClC77" 
                  frameborder="0" 
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                  allowfullscreen
                  style="width: 100%; aspect-ratio: 16/9;">
          </iframe>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <h3>Overall Framework</h3>
            <img src="./static/images/overview.png">
            <p>
              <br>
              The overall framework of VATEX processes input images and language expressions through two concurrent pathways. 
              Initially, the <span style="color: #C55A11; font-weight: 600;background: linear-gradient(120deg, rgba(197,90,17,0.1) 0%, rgba(197,90,17,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">CLIP Prior</span> module generates object queries, 
              while simultaneously, traditional Visual and Text Encoders create multiscale visual feature maps and 
              word-level text features. These visual and text features are passed into the <span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">
                Contextual Multimodal Decoder (CMD)</span> to enable multimodal interactions, yielding vision-aware text features and text-enhanced 
                visual features. We then harness vision-aware text features to ensure semantic consistency across varied textual descriptions that 
                reference the same object by employing sentence-level contrastive learning, as described in the <span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">
                  Meaning Consistency Constraint (MCC)</span> section. On the other hand, the text-enhanced visual features and the object queries generated
                   by the CLIP Prior are refined through a Masked-attention Transformer Decoder to produce the final output segmentation masks.
            </p>
          </div>

          <div class="content has-text-justified clip-prior-details">
            <h3><span style="color: #C55A11; font-weight: 600;background: linear-gradient(120deg, rgba(197,90,17,0.1) 0%, rgba(197,90,17,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">CLIP Prior</span></h3>
            <img src="./static/images/clipprior.png">
            <p>
              The CLIP Prior module leverages the powerful visual-semantic alignment capabilities of CLIP to generate initial object queries. 
              By utilizing CLIP's pre-trained knowledge, we can better localize objects mentioned in the referring expressions before detailed segmentation
              , especially for out-of-vocabulary objects.
            </p>
          </div>

          <div class="content has-text-justified cmd-details">
            <h3><span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;"">Contextutual Multimodal Decoder (CMD)</span></h3>
            <img src="./static/images/cmd.png">
            <p>
              The CMD module enables multimodal interactions by integrating visual and text features, 
              allowing the model to understand the semantic relationships between visual and textual cues.
            </p>
          </div>

          <div class="content has-text-justified mcc-details">
            <h3><span style="color: #0570C0; font-weight: 600;background: linear-gradient(120deg, rgba(5,112,192,0.1) 0%, rgba(5,112,192,0.2) 100%); padding: 2px 6px; border-radius: 4px; font-weight: 600;">Meaning Consistency Constraint (MCC)</span></h3>
            <img src="./static/images/mcc.png">
            <p>
              The MCC component ensures semantic consistency by employing contrastive learning at the sentence level. 
              This helps the model understand that different textual descriptions referring to the same object should map to similar semantic representations, 
              improving robustness and generalization.
            </p>
          </div>
        </div>
      </div>
      
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Results</h2>
        <div class="content has-text-justified">
          <img src="./static/images/table1.png" alt="empty">
          <p>
            <br>
            As shown in the table, our method achieves <span style="font-style: italic; font-weight: 600;">remarkable performance improvements</span> over state-of-the-art methods across all benchmarks on mIoU metrics. Notably, we surpass recent methods like <b>CGFormer</b> and <b>VG-LAW</b> by <span style="font-style: italic; font-weight: 600;">substantial margins</span>: <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+1.23%</span> and <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+3.11%</span> on <b>RefCOCO</b>, <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+1.46%</span> and <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+3.31%</span> on <b>RefCOCO+</b>, and <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+2.16%</span> and <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">+4.37%</span> on <b>G-Ref</b> validation splits respectively. The more complex the expressions, the greater the performance gains achieved by VATEX. Even compared to LISA, a large pre-trained vision-language model, VATEX consistently achieves an <span style="font-style: italic; font-weight: 600;">impressive</span> <span style="color: #2ea44f; text-shadow: 1px 1px 3px rgba(46,164,79,0.3);">3-5%</span> better performance across all datasets.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualizations</h2>
        <div class="content has-text-justified">
          <img src="./static/images/visualization.png" alt="empty">
          <p style="line-height: 1.5;">
            Our visualizations demonstrate VATEX's superior performance in <span style="color: #34A85A; font-weight: 800;">complex visual reasoning scenarios</span>. The model excels at:
            <br>
            <span style="color: #34A85A; font-weight: 800;">• Distinguishing Similar Objects:</span> Successfully differentiates between multiple similar instances (e.g., 1st column: identifying the seated person vs. standing person on a tennis court).
            <br>
            <span style="color: #34A85A; font-weight: 800;"">• Fine-grained Recognition:</span> Precisely locates specific items among visually similar alternatives (e.g., 2nd column: distinguishing a particular sushi plate among various food dishes).
            <br>
            <span style="color: #34A85A; font-weight: 800;">• Detailed Segmentation:</span> Captures intricate object details beyond ground truth annotations (e.g., 4th column: complete umbrella shaft segmentation).
            <br>
            <span style="color: #34A85A; font-weight: 800;">• Accurate Object Relationships:</span> Better understands complex interactions (e.g., 6th & 7th column: correctly segmenting the bear in "bear child is hugging" and "bear with 59 tag" scenario).
            <br>
            Our <span style="font-weight: 800; background: #ffd7d7; color: #8b0a1a;">instance-based architecture</span> produces smoother, more complete masks compared to LAVT and CRIS's pixel-based approach. 
            <!-- However, VATEX shows limitations with numerical ordering and indirect object references, highlighting areas for future improvement. The visualization includes failure cases in the last two columns. Best viewed in color. -->
          </p>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{Nguyen-Truong_2025_WACV,
      author    = {Nguyen-Truong, Hai and Nguyen, E-Ro and Vu, Tuan-Anh and Tran, Minh-Triet and Hua, Binh-Son and Yeung, Sai-Kit},
      title     = {Vision-Aware Text Features in Referring Image Segmentation: From Object Understanding to Context Understanding},
      booktitle = {Proceedings of the Winter Conference on Applications of Computer Vision (WACV)},
      month     = {February},
      year      = {2025},
      pages     = {4988-4998}
  }
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Thanks for the project template by <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies Project</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
